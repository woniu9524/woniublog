[{"content":" 最近项目需要接入热成像摄像头，买的海康的热成像模组，但是这个模组没有SDK，没有windows上使用demo，只有文档，所以接入很麻烦。\n初始尝试与踩坑 热成像摄像头使用UVC协议来读取，实际上opencv可以直接读取到画面。所以最初方案想用opencv读取，但是发现读取到数据只有画面没有热成像数据，查了一些资料没有找到opencv可以读取原始数据的方案，这种高级库可能不太适合读取底层数据。\n之后尝试灰度画面和温度映射关键，我读取了温度最高的点，摄像头画面有max温度，读了几个点尝试找规律，没找到，查阅资料也没有查到海康的换算算法，但是有的热成像是可以这样转换的。遂放弃方案。\n尝试Python与Node.js方案 项目中有node.js和python，首先考虑的是python方案，使用pyuvc库，但是读取帧的时候程序就会闪退，换了台电脑也不行，看issues，似乎也有人遇到类似的问题，但是这个库不怎么维护，遂放弃。node上也有一个类似的库，试了一下，也不行。这些库都是依托于libuvc这个库。\nlibuvc底层访问 所以尝试通过libuvc来更底层的访问，libuvc虽然可以跨平台，但是实际上大多数是在linux上使用的，移植到windows上比较麻烦，我使用的一个比较丝滑的教程是这个有关windows下libuvc的使用-CSDN博客 使用MSYS2来安装，很丝滑。安装好后面就使用C来进行通信就好了，libuvc有示例参考这来。\n数据结构解析 数据结构如下：\n1 2 3 总长度1315360字节 = 4640字节偏移 + 655360字节温度数据 + 655360字节图像数据 温度数据：640×512分辨率，每像素2字节，小端模式 图像数据：640×512分辨率，每像素2字节 最开始读取的时候640x512读取到的字节是655360，看起来应该是YUV数据。然后观察发现，输出的可以用的分辨率有一个640x1033，这个大小很像我的数据结构，读取试试，bingo！长度是1315360，获取成功。之后解析一下二进制数据，没问题成功获取。 最终解决方案 将C程序封装成DLL给Python调用，完成了整个热成像模组的接入过程。\n以下是核心代码片段：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 #include \u0026#34;library.h\u0026#34; #include \u0026#34;libuvc/libuvc.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;time.h\u0026gt; #ifdef _WIN32 #include \u0026lt;direct.h\u0026gt; // 包含_mkdir函数 #define mkdir(dir, mode) _mkdir(dir) // 在Windows下重定义mkdir #else #include \u0026lt;sys/stat.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #endif /* 定义数据结构 */ #define FRAME_OFFSET 4640 #define FRAME_WIDTH 640 #define FRAME_HEIGHT 512 #define PIXEL_SIZE 2 #define FRAME_SIZE (FRAME_WIDTH * FRAME_HEIGHT * PIXEL_SIZE) #define TOTAL_SIZE (FRAME_OFFSET + FRAME_SIZE * 2) // 温度数据 + 图像数据 /* 修改保存数据相关定义 */ #define SAVE_DIR \u0026#34;./thermal_data\u0026#34; // 全局变量 static uvc_context_t *ctx = NULL; static uvc_device_t *dev = NULL; static uvc_device_handle_t *devh = NULL; static uvc_stream_ctrl_t ctrl; static int is_streaming = 0; // 用户回调函数指针 static FrameCallback user_callback = NULL; static void* user_data = NULL; // 内部回调函数，将被传递给libuvc static void internal_callback(uvc_frame_t *frame, void *ptr) { printf(\u0026#34;已接收帧：序列=%u, 数据字节=%lu\\n\u0026#34;, frame-\u0026gt;sequence, frame-\u0026gt;data_bytes); // 如果用户提供了回调函数，调用它 if (user_callback) { user_callback((unsigned char*)frame-\u0026gt;data, (int)frame-\u0026gt;data_bytes, frame-\u0026gt;sequence); } } // 保存帧数据到文件 UVC_THERMAL_API void save_frame_to_file(const char *prefix, void *data, int size, unsigned int frame_seq) { char filename[256]; char timestamp[32]; time_t now; struct tm *timeinfo; // 创建保存目录 #ifdef _WIN32 mkdir(SAVE_DIR, 0); #else struct stat st = {0}; if (stat(SAVE_DIR, \u0026amp;st) == -1) { mkdir(SAVE_DIR, 0755); } #endif // 获取当前时间作为文件名 time(\u0026amp;now); timeinfo = localtime(\u0026amp;now); strftime(timestamp, sizeof(timestamp), \u0026#34;%Y%m%d_%H%M%S\u0026#34;, timeinfo); // 构建文件名 #ifdef _WIN32 snprintf(filename, sizeof(filename), \u0026#34;%s\\\\frame_%s_%s_%u.raw\u0026#34;, SAVE_DIR, prefix, timestamp, frame_seq); #else snprintf(filename, sizeof(filename), \u0026#34;%s/frame_%s_%s_%u.raw\u0026#34;, SAVE_DIR, prefix, timestamp, frame_seq); #endif // 保存数据 FILE *fp = fopen(filename, \u0026#34;wb\u0026#34;); if (!fp) { printf(\u0026#34;错误：无法打开文件 %s 进行写入\\n\u0026#34;, filename); return; } size_t written = fwrite(data, 1, size, fp); fclose(fp); if (written == size) { printf(\u0026#34;已保存数据到 %s (%d 字节)\\n\u0026#34;, filename, size); } else { printf(\u0026#34;错误：未能将所有数据写入 %s (%zu / %d 字节已写入)\\n\u0026#34;, filename, written, size); } } // 初始化相机 UVC_THERMAL_API int init_camera(void) { uvc_error_t res; // 如果已经初始化，先关闭 if (ctx != NULL) { close_camera(); } // 初始化UVC服务上下文 res = uvc_init(\u0026amp;ctx, NULL); if (res \u0026lt; 0) { uvc_perror(res, \u0026#34;uvc_init\u0026#34;); return res; } printf(\u0026#34;UVC已初始化\\n\u0026#34;); // 定位第一个连接的UVC设备 res = uvc_find_device(ctx, \u0026amp;dev, 0, 0, NULL); if (res \u0026lt; 0) { uvc_perror(res, \u0026#34;uvc_find_device\u0026#34;); uvc_exit(ctx); ctx = NULL; return res; } printf(\u0026#34;已找到设备\\n\u0026#34;); // 尝试打开设备 res = uvc_open(dev, \u0026amp;devh); if (res \u0026lt; 0) { uvc_perror(res, \u0026#34;uvc_open\u0026#34;); uvc_unref_device(dev); dev = NULL; uvc_exit(ctx); ctx = NULL; return res; } printf(\u0026#34;设备已打开\\n\u0026#34;); // 打印设备信息 uvc_print_diag(devh, stderr); return 0; } // 启动相机流 UVC_THERMAL_API int start_camera(FrameCallback callback, void* user_ptr) { uvc_error_t res; if (devh == NULL) { printf(\u0026#34;错误：相机未初始化，请先调用init_camera()\\n\u0026#34;); return -1; } if (is_streaming) { printf(\u0026#34;相机已经在流式传输中\\n\u0026#34;); return 0; } // 保存用户回调和数据 user_callback = callback; user_data = user_ptr; // 获取格式描述符 const uvc_format_desc_t *format_desc = uvc_get_format_descs(devh); enum uvc_frame_format frame_format = UVC_FRAME_FORMAT_YUYV; int width = 640; int height = 512; int fps = 30; // 查找对应的frame descriptor const uvc_frame_desc_t *frame_desc = format_desc-\u0026gt;frame_descs; int found_frame = 0; // 尝试找到640x1033的帧描述符 while (frame_desc) { if (frame_desc-\u0026gt;wWidth == 640 \u0026amp;\u0026amp; frame_desc-\u0026gt;wHeight == 1033) { fps = 10000000 / frame_desc-\u0026gt;dwDefaultFrameInterval; width = 640; height = 1033; found_frame = 1; printf(\u0026#34;\\n找到帧格式：%dx%d %dfps\\n\u0026#34;, width, height, fps); break; } frame_desc = frame_desc-\u0026gt;next; } if (!found_frame) { // 如果没找到，使用默认的640x512 printf(\u0026#34;\\n使用默认帧格式：640x512\\n\u0026#34;); width = 640; height = 512; } // 设置流控制参数 memset(\u0026amp;ctrl, 0, sizeof(ctrl)); // 尝试协商流参数 res = uvc_get_stream_ctrl_format_size( devh, \u0026amp;ctrl, frame_format, width, height, fps ); // 打印结果 uvc_print_stream_ctrl(\u0026amp;ctrl, stderr); if (res \u0026lt; 0) { uvc_perror(res, \u0026#34;get_mode\u0026#34;); // 尝试使用另一种方式设置流控制 printf(\u0026#34;\\n尝试替代方法设置流控制...\\n\u0026#34;); // 手动设置流控制参数 memset(\u0026amp;ctrl, 0, sizeof(ctrl)); ctrl.bmHint = 1; ctrl.bFormatIndex = 1; // YUYV格式索引 ctrl.bFrameIndex = found_frame ? 4 : 2; // 使用找到的帧索引或默认值 ctrl.dwFrameInterval = 333333; // 30 fps ctrl.wDelay = 0; ctrl.dwMaxVideoFrameSize = found_frame ? 1315360 : 655360; // 根据帧大小设置 ctrl.dwMaxPayloadTransferSize = 14336; ctrl.bInterfaceNumber = 1; // 打印手动设置的流控制参数 printf(\u0026#34;\\n手动设置的流控制参数：\\n\u0026#34;); uvc_print_stream_ctrl(\u0026amp;ctrl, stderr); } printf(\u0026#34;\\n尝试格式：自定义 %dx%d 30fps (总大小：%d 字节)\\n\u0026#34;, FRAME_WIDTH, FRAME_HEIGHT, TOTAL_SIZE); // 开始视频流 res = uvc_start_streaming(devh, \u0026amp;ctrl, internal_callback, user_data, 0); if (res \u0026lt; 0) { uvc_perror(res, \u0026#34;start_streaming\u0026#34;); return res; } printf(\u0026#34;正在流式传输...\\n\u0026#34;); is_streaming = 1; // 启用自动曝光 printf(\u0026#34;启用自动曝光...\\n\u0026#34;); const uint8_t UVC_AUTO_EXPOSURE_MODE_AUTO = 2; res = uvc_set_ae_mode(devh, UVC_AUTO_EXPOSURE_MODE_AUTO); if (res == UVC_SUCCESS) { printf(\u0026#34; ... 已启用自动曝光\\n\u0026#34;); } else if (res == UVC_ERROR_PIPE) { // 尝试光圈优先模式 printf(\u0026#34; ... 不支持完整AE，尝试光圈优先模式\\n\u0026#34;); const uint8_t UVC_AUTO_EXPOSURE_MODE_APERTURE_PRIORITY = 8; res = uvc_set_ae_mode(devh, UVC_AUTO_EXPOSURE_MODE_APERTURE_PRIORITY); if (res \u0026lt; 0) { uvc_perror(res, \u0026#34; ... uvc_set_ae_mode无法启用光圈优先模式\u0026#34;); } else { printf(\u0026#34; ... 已启用光圈优先自动曝光模式\\n\u0026#34;); } } else { uvc_perror(res, \u0026#34; ... uvc_set_ae_mode无法启用自动曝光模式\u0026#34;); } return 0; } // 停止相机流 UVC_THERMAL_API void stop_camera(void) { if (devh != NULL \u0026amp;\u0026amp; is_streaming) { uvc_stop_streaming(devh); is_streaming = 0; printf(\u0026#34;流式传输已停止\\n\u0026#34;); } } // 关闭相机 UVC_THERMAL_API void close_camera(void) { // 如果正在流式传输，先停止 if (is_streaming) { stop_camera(); } // 关闭设备句柄 if (devh != NULL) { uvc_close(devh); devh = NULL; printf(\u0026#34;设备已关闭\\n\u0026#34;); } // 释放设备描述符 if (dev != NULL) { uvc_unref_device(dev); dev = NULL; } // 关闭UVC上下文 if (ctx != NULL) { uvc_exit(ctx); ctx = NULL; printf(\u0026#34;UVC已退出\\n\u0026#34;); } } // 原始的hello函数 UVC_THERMAL_API void hello(void) { printf(\u0026#34;Hello, World!\\n\u0026#34;); } 最后，海康，你妈没了，浪费我一个星期。\n参考资料 libuvc GitHub仓库 有关windows下libuvc的使用-CSDN博客 UVC协议规范 ","date":"2025-02-28T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/windows%E4%B8%8A%E6%8E%A5%E5%85%A5uvc%E7%83%AD%E6%88%90%E5%83%8F%E6%A8%A1%E7%BB%84/cover_hu_a19cd2cdd91ad0d3.png","permalink":"https://woniu9524.github.io/woniublog/p/windows%E4%B8%8A%E6%8E%A5%E5%85%A5uvc%E7%83%AD%E6%88%90%E5%83%8F%E6%A8%A1%E7%BB%84/","title":"Windows上接入UVC热成像模组"},{"content":" Ollama通常被设计为本地使用，但有时为了便于分享会直接部署在公网服务器上。这种做法可能导致任何互联网用户都能调用你的Ollama服务。本文将探讨这个安全隐患以及相关的防护建议。\n一、通过Fofa搜索Ollama API Fofa是一个强大的互联网设备和服务搜索引擎，可以帮助发现公开的API服务。通过特定的搜索语法，我们可以精准定位暴露在互联网上的Ollama API服务。\nFofa搜索步骤 访问Fofa官网，使用以下搜索语句： 1 app=\u0026#34;Ollama\u0026#34; \u0026amp;\u0026amp; is_domain=false 这个搜索语句包含两个关键条件：\napp=\u0026ldquo;Ollama\u0026rdquo;：筛选Ollama相关服务 is_domain=false：排除域名服务，只显示直接暴露的API端点 二、使用Shodan搜索Ollama API Shodan是另一个专门用于发现互联网设备和服务的搜索引擎。它可以帮助我们找到运行中的Ollama实例。\nShodan搜索方法 访问Shodan官网，输入以下搜索语句： 1 Ollama is running 搜索结果会返回包含Ollama API实例的IP地址和端口信息。\n结语 不要把Ollama直接部署在公网上，另外部署在公网上的服务要考虑安全问题，很容易被扫出来。\n","date":"2025-02-14T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/%E6%90%9C%E7%B4%A2%E6%9A%B4%E9%9C%B2%E5%9C%A8%E5%85%AC%E7%BD%91%E4%B8%AD%E7%9A%84ollama%E4%BB%A5%E5%8F%8Aollama%E5%AE%89%E5%85%A8%E9%83%A8%E7%BD%B2%E5%BB%BA%E8%AE%AE/cover_hu_8aa8d69bc65b0f9a.png","permalink":"https://woniu9524.github.io/woniublog/p/%E6%90%9C%E7%B4%A2%E6%9A%B4%E9%9C%B2%E5%9C%A8%E5%85%AC%E7%BD%91%E4%B8%AD%E7%9A%84ollama%E4%BB%A5%E5%8F%8Aollama%E5%AE%89%E5%85%A8%E9%83%A8%E7%BD%B2%E5%BB%BA%E8%AE%AE/","title":"搜索暴露在公网中的Ollama，以及Ollama安全部署建议"},{"content":" 最近在做一个小鼠步态分析的项目，需要用高速摄像头记录小白鼠的运动轨迹。本来，直接用前端的 getUserMedia ，结果发现事情并没有那么简单\u0026hellip;\n踩坑 一开始直接在前端录制：\n1 2 3 4 5 6 7 const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 1280, height: 720, frameRate: 120 } }); 然后发现：TMD视频有问题，还要用ffmpeg重新编码一遍，还有画质不是很好控制，我查了资料可能达不到120fps录制\n核心实现思路 整个方案的架构是这样的：\nElectron作为应用框架 FFmpeg负责视频采集和编码 WebSocket实现实时预览 同时进行预览流和存储流的处理 FFmpeg的参数设置，我把它分成了两路：\n一路用于实时预览，追求低延迟 另一路用于存储，追求高质量 代码实现的关键点 1. 启动FFmpeg进程 1 2 3 4 5 6 7 8 const args = [ \u0026#39;-f\u0026#39;, \u0026#39;dshow\u0026#39;, \u0026#39;-video_size\u0026#39;, \u0026#39;1280x720\u0026#39;, \u0026#39;-framerate\u0026#39;, \u0026#39;120\u0026#39;, // 还有超级多参数... ]; const ffmpeg = spawn(ffmpegPath, args); 2. 视频流分流处理 1 -filter_complex \u0026#34;split=2[preview][record]\u0026#34; 3. 预览流优化 为了实现低延迟预览，我用了一堆\u0026quot;降质但提速\u0026quot;的参数：\n1 2 3 -preset ultrafast -tune zerolatency -x264opts \u0026#34;no-scenecut\u0026#34; 结语 之前希望尽量在electron里处理用的node，最后弄的很乱很麻烦。后面再用的时候就换python吧，之前处理的太差了，尤其是对摄像头资源占用的问题没解决好。\n","date":"2024-11-15T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/%E5%A6%82%E4%BD%95%E7%94%A8electron%E5%AE%9E%E7%8E%B0120fps%E9%AB%98%E5%B8%A7%E7%8E%87%E8%A7%86%E9%A2%91%E9%87%87%E9%9B%86/cover_hu_e95a4276bf860a84.jpg","permalink":"https://woniu9524.github.io/woniublog/p/%E5%A6%82%E4%BD%95%E7%94%A8electron%E5%AE%9E%E7%8E%B0120fps%E9%AB%98%E5%B8%A7%E7%8E%87%E8%A7%86%E9%A2%91%E9%87%87%E9%9B%86/","title":"如何用Electron实现120fps高帧率视频采集"},{"content":" 今天有遇到，组件不能正常更新的问题，第二次使用key来实现组件刷新，在react中我记得也有相同的用法。\n1. 组件复用机制 1.1 基本概念 Vue 为了提高性能，会尽可能地复用已经创建的组件实例。当一个组件的显示状态发生变化时（比如通过 v-if 或 v-show），Vue 不会立即销毁并重新创建组件，而是尽可能地复用现有组件实例。\n1.2 示例说明 1 2 3 4 5 6 \u0026lt;template\u0026gt; \u0026lt;div\u0026gt; \u0026lt;!-- 当切换不同的 userId 时，UserProfile 组件会被复用 --\u0026gt; \u0026lt;UserProfile v-if=\u0026#34;showProfile\u0026#34; :user-id=\u0026#34;userId\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; 在这个例子中，即使 userId 改变，组件实例也会被复用，这意味着：\n组件的生命周期钩子不会重新触发 组件的 data 不会重新初始化 仅会更新组件的 props 1.3 复用的优缺点 优点：\n提高性能，减少组件创建和销毁的开销 保持组件状态，避免不必要的重渲染 减少内存占用 缺点：\n可能导致状态残留 某些情况下不符合业务需求 可能造成数据更新不及时 2. key 属性 2.1 作用 key 属性是 Vue 中用于标识虚拟 DOM 节点的唯一标识符。它可以：\n强制组件重新渲染 管理可复用的元素 触发过渡效果 2.2 使用场景 1 2 3 4 5 6 7 8 9 10 \u0026lt;!-- 基本用法 --\u0026gt; \u0026lt;template\u0026gt; \u0026lt;!-- 列表渲染 --\u0026gt; \u0026lt;div v-for=\u0026#34;item in items\u0026#34; :key=\u0026#34;item.id\u0026#34;\u0026gt; {{ item.name }} \u0026lt;/div\u0026gt; \u0026lt;!-- 强制组件重新渲染 --\u0026gt; \u0026lt;UserProfile :key=\u0026#34;userId\u0026#34; :user-id=\u0026#34;userId\u0026#34; /\u0026gt; \u0026lt;/template\u0026gt; 2.3 工作原理 Virtual DOM 比对：\nVue 使用 key 来标识节点的唯一性 在更新时，比较新旧节点的 key 值 如果 key 不同，则认为是不同的元素，会重新渲染 组件更新流程：\n1 2 3 4 5 6 7 8 9 // 伪代码示例 if (oldVnode.key !== newVnode.key) { // 销毁旧组件，创建新组件 destroyComponent(oldVnode); createComponent(newVnode); } else { // 复用组件，更新属性 updateComponent(oldVnode, newVnode); } 3. 实际应用示例 3.1 不使用 key 的问题 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;template\u0026gt; \u0026lt;UserProfile v-if=\u0026#34;showProfile\u0026#34; :user-id=\u0026#34;userId\u0026#34; /\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script setup\u0026gt; import { ref } from \u0026#39;vue\u0026#39;; const userId = ref(1); const showProfile = ref(true); // 当 userId 变化时，组件会被复用 // 可能导致数据不更新或状态混乱 \u0026lt;/script\u0026gt; 3.2 使用 key 的正确方式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026lt;template\u0026gt; \u0026lt;UserProfile v-if=\u0026#34;showProfile\u0026#34; :key=\u0026#34;userId\u0026#34; :user-id=\u0026#34;userId\u0026#34; /\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script setup\u0026gt; import { ref } from \u0026#39;vue\u0026#39;; const userId = ref(1); const showProfile = ref(true); // 当 userId 变化时，组件会重新创建 // 确保数据完全刷新，状态重置 \u0026lt;/script\u0026gt; 4. 最佳实践 4.1 何时使用 key 列表渲染中（v-for） 需要强制组件重新渲染时 使用动态组件时 需要触发过渡效果时 4.2 key 的选择 1 2 3 4 5 6 7 8 9 \u0026lt;!-- 推荐的 key 值选择 --\u0026gt; \u0026lt;!-- 1. 使用唯一标识符 --\u0026gt; \u0026lt;div v-for=\u0026#34;item in items\u0026#34; :key=\u0026#34;item.id\u0026#34;\u0026gt; \u0026lt;!-- 2. 使用索引（当列表是静态的） --\u0026gt; \u0026lt;div v-for=\u0026#34;(item, index) in items\u0026#34; :key=\u0026#34;index\u0026#34;\u0026gt; \u0026lt;!-- 3. 使用组合值 --\u0026gt; \u0026lt;Component :key=\u0026#34;`${userId}-${type}`\u0026#34;\u0026gt; 4.3 注意事项 key 必须是唯一的 不要使用随机数作为 key 避免使用可能会变化的值作为 key 在使用 v-for 时总是建议提供 key 5. 性能考虑 合理使用 key 可以提高更新效率 避免使用索引作为 key（在动态列表中） 在大量数据渲染时，key 的选择会影响性能 ","date":"2024-11-05T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/vue%E7%BB%84%E4%BB%B6%E5%A4%8D%E7%94%A8%E6%9C%BA%E5%88%B6%E4%B8%8Ekey%E5%B1%9E%E6%80%A7%E5%88%B7%E6%96%B0/cover_hu_81696289db7f19d3.png","permalink":"https://woniu9524.github.io/woniublog/p/vue%E7%BB%84%E4%BB%B6%E5%A4%8D%E7%94%A8%E6%9C%BA%E5%88%B6%E4%B8%8Ekey%E5%B1%9E%E6%80%A7%E5%88%B7%E6%96%B0/","title":"Vue组件复用机制与key属性刷新"},{"content":"依赖安装 在electron项目中集成ffmpeg，需要安装@ffmpeg-installer/ffmpeg和fluent-ffmpeg\n1 npm install @ffmpeg-installer/ffmpeg fluent-ffmpeg --save 其中@ffmpeg-installer会预编译不同平台的二进制文件，通过 npm 包管理和分发这些二进制文件，运行时根据平台选择正确的二进制文件，并且能正确处理文件路径和权限。我们也可以参照类似的方法打包任意命令行工具。而fluent-ffmpeg 是一个 Node.js 的 FFmpeg 命令行封装库。\n使用 这里我使用的egg框架，使用的时候需要注意编译的问题。在开发环境中，使用ffmpegInstaller.path获取就可以了。编译后ffmpeg在resources\\app.asar.unpacked目录下，所以我的需对代码做些修改。如果不是egg，或许情况稍有不同，具体看目录结构和报错就好了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 // services/video.js \u0026#39;use strict\u0026#39;; const { Service } = require(\u0026#39;ee-core\u0026#39;); const ffmpeg = require(\u0026#39;fluent-ffmpeg\u0026#39;); const ffmpegInstaller = require(\u0026#39;@ffmpeg-installer/ffmpeg\u0026#39;); const ffprobeInstaller = require(\u0026#39;@ffprobe-installer/ffprobe\u0026#39;); const Log = require(\u0026#39;ee-core/log\u0026#39;); class VideoService extends Service { constructor(ctx) { super(ctx); this.initializeFfmpeg(); } initializeFfmpeg() { try { // 获取正确的 ffmpeg 路径 const getFfmpegPath = () =\u0026gt; { if (process.env.EE_SERVER_ENV === \u0026#39;local\u0026#39;) { return ffmpegInstaller.path; } // 生产环境 const { app } = require(\u0026#39;electron\u0026#39;); const path = require(\u0026#39;path\u0026#39;); // 方案1：如果使用 asarUnpack const appPath = app.getAppPath(); const unpackedPath = appPath.replace(\u0026#39;app.asar\u0026#39;, \u0026#39;app.asar.unpacked\u0026#39;); return path.join( unpackedPath, \u0026#39;node_modules\u0026#39;, \u0026#39;@ffmpeg-installer\u0026#39;, \u0026#39;win32-x64\u0026#39;, \u0026#39;ffmpeg.exe\u0026#39; ); }; // 获取正确的 ffprobe 路径 const getFfprobePath = () =\u0026gt; { if (process.env.EE_SERVER_ENV === \u0026#39;local\u0026#39;) { return ffprobeInstaller.path; } const { app } = require(\u0026#39;electron\u0026#39;); const path = require(\u0026#39;path\u0026#39;); // 方案1：如果使用 asarUnpack const appPath = app.getAppPath(); const unpackedPath = appPath.replace(\u0026#39;app.asar\u0026#39;, \u0026#39;app.asar.unpacked\u0026#39;); return path.join( unpackedPath, \u0026#39;node_modules\u0026#39;, \u0026#39;@ffprobe-installer\u0026#39;, \u0026#39;win32-x64\u0026#39;, \u0026#39;ffprobe.exe\u0026#39; ); }; const ffmpegPath = getFfmpegPath(); const ffprobePath = getFfprobePath(); Log.info(\u0026#39;FFmpeg path:\u0026#39;, ffmpegPath); Log.info(\u0026#39;FFprobe path:\u0026#39;, ffprobePath); // 检查文件是否存在 const fs = require(\u0026#39;fs\u0026#39;); if (!fs.existsSync(ffmpegPath)) { throw new Error(`FFmpeg executable not found at: ${ffmpegPath}`); } if (!fs.existsSync(ffprobePath)) { throw new Error(`FFprobe executable not found at: ${ffprobePath}`); } // 设置路径 ffmpeg.setFfmpegPath(ffmpegPath); ffmpeg.setFfprobePath(ffprobePath); // 测试 FFmpeg 是否可用 ffmpeg.getAvailableFormats((err, formats) =\u0026gt; { if (err) { Log.error(\u0026#39;FFmpeg test failed:\u0026#39;, err); throw err; } Log.info(\u0026#39;FFmpeg initialized successfully. Available formats:\u0026#39;, Object.keys(formats).length); }); } catch (error) { Log.error(\u0026#39;Failed to initialize FFmpeg:\u0026#39;, error); throw error; } } // ... [其他代码部分] } VideoService.toString = () =\u0026gt; \u0026#39;[class VideoService]\u0026#39;; module.exports = VideoService; ","date":"2024-10-31T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/electron%E9%9B%86%E6%88%90ffmpeg/cover_hu_1febc640691670b9.png","permalink":"https://woniu9524.github.io/woniublog/p/electron%E9%9B%86%E6%88%90ffmpeg/","title":"Electron集成FFmpeg"},{"content":" 需求\n项目中有些复杂操作，后面考虑使用python来处理。其实更好的跨语言方案我觉得用tauri wails之类的比较好，看起来会专业一些。不过我打算用electron，其他语言只是作为一个服务。我记得我在大学时候写electron就特别想把python集成进来，但是当时技术比较菜，也没想好的办法，这次使用egg来做，它集成了跨语言的解决方案。我看了一下源码，实际解决方案也很朴素，没有我想的会很高深。实际上就是实现了一个进程的控制。例如我打包一个fastapi程序，一个没有窗口的程序。electron中会启动这个进程，可以进行一些配置比如端口等，然后就正常通信，程序关闭的时候也关闭服务。\n操作\npython端 新建一个python文件夹放入相关代码，然后配置打包脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 from cx_Freeze import setup, Executable # 定义额外需要包含的包 includes = [ \u0026#34;uvicorn\u0026#34;, \u0026#34;uvicorn.loops\u0026#34;, \u0026#34;uvicorn.loops.auto\u0026#34;, \u0026#34;uvicorn.protocols\u0026#34;, \u0026#34;uvicorn.protocols.http\u0026#34;, \u0026#34;uvicorn.lifespan\u0026#34;, \u0026#34;uvicorn.lifespan.on\u0026#34;, \u0026#34;uvicorn.lifespan.off\u0026#34;, \u0026#34;uvicorn.logging\u0026#34;, \u0026#34;fastapi\u0026#34;, \u0026#34;starlette\u0026#34;, \u0026#34;pydantic\u0026#34; ] # 创建可执行文件的配置 executableApp = Executable( script=\u0026#34;main.py\u0026#34;, target_name=\u0026#34;pyapp\u0026#34;, ) # 打包的参数配置 options = { \u0026#34;build_exe\u0026#34;: { \u0026#34;build_exe\u0026#34;: \u0026#34;./dist/\u0026#34;, \u0026#34;excludes\u0026#34;: [\u0026#34;*.txt\u0026#34;], \u0026#34;optimize\u0026#34;: 2, \u0026#34;includes\u0026#34;: includes, \u0026#34;packages\u0026#34;: includes, \u0026#34;zip_include_packages\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;zip_exclude_packages\u0026#34;: \u0026#34;\u0026#34;, } } setup( name=\u0026#34;pyapp\u0026#34;, version=\u0026#34;1.0\u0026#34;, description=\u0026#34;python app\u0026#34;, options=options, executables=[executableApp] ) electron中配置启动\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 async createPythonServer() { // method 1: Use the default Settings //const entity = await Cross.run(serviceName); // method 2: Use custom configuration const serviceName = \u0026#34;python\u0026#34;; const opt = { name: \u0026#39;pyapp\u0026#39;, cmd: path.join(Ps.getExtraResourcesDir(), \u0026#39;py\u0026#39;, \u0026#39;pyapp\u0026#39;), directory: path.join(Ps.getExtraResourcesDir(), \u0026#39;py\u0026#39;), args: [\u0026#39;--port=10305\u0026#39;], windowsExtname: true, appExit: true, } const entity = await Cross.run(serviceName, opt); Log.info(\u0026#39;server name:\u0026#39;, entity.name); Log.info(\u0026#39;server config:\u0026#39;, entity.config); Log.info(\u0026#39;server url:\u0026#39;, entity.getUrl()); return; } } ","date":"2024-10-23T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/electron%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%80%9A%E4%BF%A1/cover_hu_a8e3403fe4b4ed36.png","permalink":"https://woniu9524.github.io/woniublog/p/electron%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%80%9A%E4%BF%A1/","title":"Electron跨语言通信"},{"content":"NextAuth配置教程 尝试使用Prisma作为数据库适配器,并配置GitHub OAuth和邮箱密码登录。\n简述next-auth配置 next-auth是用来给next项目增加认证的，既可以配置邮箱也可以配置GitHub，Google等授权认证，整个使用流程相对简单，而且似乎配置GitHub认证好像其实数据库都可以不要。暂时还处于摸索阶段，勉勉强强配置好了登录，实际上当前配置的逻辑还有点问题，例如GitHub授权后user表中有邮箱，这时候如果邮箱注册会冲突，不过处理起来也还好，暂时先不做了。\n关于使用，客户端中提供useSession拿到用户信息，服务端中通过getServerSession获取用户信息。总的来说配置和使用上手难度还是比较容易的。但是配置Google的时候总是报错，测试可能是服务器连不上，但是我开了全局也不行，后面再试试。 下面用配置好的代码，用Claude生成了大致的步骤\n步骤1: 安装依赖 首先,安装必要的依赖:\n1 2 npm install next-auth @prisma/client @next-auth/prisma-adapter bcryptjs npm install prisma --save-dev 步骤2: 配置环境变量 在.env文件中添加以下环境变量:\n1 2 3 4 5 DATABASE_URL=\u0026#34;your_database_url_here\u0026#34; NEXTAUTH_URL=http://localhost:3000 NEXTAUTH_SECRET=your_nextauth_secret_here GITHUB_ID=your_github_client_id GITHUB_SECRET=your_github_client_secret 步骤3: 设置Prisma 初始化Prisma并创建数据库schema:\n1 npx prisma init 在prisma/schema.prisma文件中定义您的模型。这里是一个基本的用户模型示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 datasource db { provider = \u0026#34;postgresql\u0026#34; url = env(\u0026#34;DATABASE_URL\u0026#34;) } generator client { provider = \u0026#34;prisma-client-js\u0026#34; } model Account { id String @id @default(cuid()) userId String type String provider String providerAccountId String refresh_token String? @db.Text access_token String? @db.Text expires_at Int? token_type String? scope String? id_token String? @db.Text session_state String? user User @relation(fields: [userId], references: [id], onDelete: Cascade) @@unique([provider, providerAccountId]) } model Session { id String @id @default(cuid()) sessionToken String @unique userId String expires DateTime user User @relation(fields: [userId], references: [id], onDelete: Cascade) } model User { id String @id @default(cuid()) name String? email String @unique password String? emailVerified DateTime? image String? accounts Account[] sessions Session[] createdAt DateTime @default(now()) updatedAt DateTime @updatedAt } model VerificationToken { identifier String token String @unique expires DateTime @@unique([identifier, token]) } 然后,运行以下命令来创建数据库表:\n1 npx prisma db push 步骤4: 创建NextAuth配置文件 创建lib/auth.ts文件并添加以下内容:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 import {NextAuthOptions} from \u0026#34;next-auth\u0026#34;; import {PrismaAdapter} from \u0026#34;@next-auth/prisma-adapter\u0026#34;; import {PrismaClient} from \u0026#34;@prisma/client\u0026#34;; import CredentialsProvider from \u0026#34;next-auth/providers/credentials\u0026#34;; import GitHubProvider from \u0026#34;next-auth/providers/github\u0026#34;; import bcrypt from \u0026#34;bcryptjs\u0026#34;; // 创建 Prisma 客户端实例 const prisma = new PrismaClient(); // 定义 NextAuth 的配置选项 export const authOptions: NextAuthOptions = { // 使用 Prisma 适配器来处理数据库操作 adapter: PrismaAdapter(prisma), // 配置身份验证提供者 providers: [ // 使用邮箱密码的凭证提供者 CredentialsProvider({ name: \u0026#34;Credentials\u0026#34;, credentials: { email: {label: \u0026#34;Email\u0026#34;, type: \u0026#34;text\u0026#34;}, password: {label: \u0026#34;Password\u0026#34;, type: \u0026#34;password\u0026#34;} }, // 验证用户凭证 async authorize(credentials) { if (!credentials?.email || !credentials?.password) { throw new Error(\u0026#34;Missing credentials\u0026#34;); } // 在数据库中查找用户 const user = await prisma.user.findUnique({ where: {email: credentials.email} }); if (!user || !user.password) { throw new Error(\u0026#34;User not found\u0026#34;); } // 验证密码 const isPasswordValid = await bcrypt.compare(credentials.password, user.password); if (!isPasswordValid) { throw new Error(\u0026#34;Invalid password\u0026#34;); } return user; } }), // GitHub OAuth 提供者 GitHubProvider({ clientId: process.env.GITHUB_ID!, clientSecret: process.env.GITHUB_SECRET!, }), ], // 自定义认证页面路径 pages: { signIn: \u0026#39;/auth/signin\u0026#39;, signOut: \u0026#39;/auth/signout\u0026#39;, error: \u0026#39;/auth/error\u0026#39;, verifyRequest: \u0026#39;/auth/verify-request\u0026#39;, }, // 使用 JWT 策略管理会话 session: { strategy: \u0026#34;jwt\u0026#34;, }, // 回调函数配置 callbacks: { // 登录回调 async signIn({user, account, profile, email, credentials}) { try { console.log(\u0026#34;Sign In Callback:\u0026#34;, {user, account, profile, email}); // 处理 GitHub 登录 if (account \u0026amp;\u0026amp; account.provider === \u0026#39;github\u0026#39; \u0026amp;\u0026amp; profile) { // 查找现有用户 const existingUser = await prisma.user.findFirst({ where: { OR: [ {email: profile.email}, {accounts: {some: {providerAccountId: account.providerAccountId}}} ] }, include: {accounts: true} }); if (existingUser) { // 如果用户存在但没有 GitHub 账号，则添加 GitHub 账号 if (!existingUser.accounts.some(acc =\u0026gt; acc.provider === \u0026#39;github\u0026#39;)) { await prisma.account.create({ data: { userId: existingUser.id, type: account.type, provider: account.provider, providerAccountId: account.providerAccountId, access_token: account.access_token, token_type: account.token_type, scope: account.scope, } }); } return true; } // 如果用户不存在，创建新用户 await prisma.user.create({ data: { name: profile.name || (profile as any).login || \u0026#39;Unknown\u0026#39;, email: profile.email || `${account.providerAccountId}@github.com`, accounts: { create: { type: account.type, provider: account.provider, providerAccountId: account.providerAccountId, access_token: account.access_token, token_type: account.token_type, scope: account.scope, } } } }); } return true; } catch (error) { console.error(\u0026#34;Error in signIn callback:\u0026#34;, error); return false; } }, // JWT 回调 async jwt({token, user, account}) { if (user) { token.id = user.id; } if (account \u0026amp;\u0026amp; account.access_token) { token.accessToken = account.access_token; } return token; }, // 会话回调 async session({session, token}) { if (session.user) { (session.user as any).id = token.id as string; (session as any).accessToken = token.accessToken; } return session; }, // 重定向回调 async redirect({url, baseUrl}) { // 允许相对回调 URL if (url.startsWith(\u0026#34;/\u0026#34;)) return `${baseUrl}${url}` // 允许同源的回调 URL else if (new URL(url).origin === baseUrl) return url return baseUrl }, }, // 认证事件处理 events: { async signIn(message) { console.log(\u0026#34;User signed in:\u0026#34;, message) }, async signOut(message) { console.log(\u0026#34;User signed out:\u0026#34;, message) }, async createUser(message) { console.log(\u0026#34;User created:\u0026#34;, message) }, async linkAccount(message) { console.log(\u0026#34;Account linked:\u0026#34;, message) }, async session(message) { console.log(\u0026#34;Session created:\u0026#34;, message) }, }, // 开发环境启用调试模式 debug: process.env.NODE_ENV === \u0026#39;development\u0026#39;, // 日志配置 logger: { error(code, metadata) { console.error(code, metadata); }, warn(code) { console.warn(code); }, debug(code, metadata) { console.log(code, metadata); }, }, }; 步骤5: 创建API路由 创建app/api/auth/[...nextauth]/route.ts文件:\n1 2 3 4 5 6 import NextAuth from \u0026#34;next-auth\u0026#34;; import { authOptions } from \u0026#34;@/lib/auth\u0026#34;; const handler = NextAuth(authOptions); export { handler as GET, handler as POST }; 步骤6: 创建注册API 创建app/api/auth/register/route.ts文件:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import { NextResponse } from \u0026#39;next/server\u0026#39;; import bcrypt from \u0026#39;bcryptjs\u0026#39;; import { PrismaClient } from \u0026#39;@prisma/client\u0026#39;; const prisma = new PrismaClient(); export async function POST(req: Request) { try { const { name, email, password } = await req.json(); const existingUser = await prisma.user.findUnique({ where: { email } }); if (existingUser) { return NextResponse.json({ error: \u0026#34;User already exists\u0026#34; }, { status: 400 }); } const hashedPassword = await bcrypt.hash(password, 10); const user = await prisma.user.create({ data: { name, email, password: hashedPassword }, }); return NextResponse.json({ message: \u0026#34;User created successfully\u0026#34; }, { status: 201 }); } catch (error) { console.error(\u0026#34;Registration error:\u0026#34;, error); return NextResponse.json({ error: \u0026#34;An error occurred during registration\u0026#34; }, { status: 500 }); } } 步骤7: 创建登录和注册页面 创建app/auth/signin/page.tsx和app/auth/register/page.tsx文件,实现登录和注册表单。\n步骤8: 在应用中使用NextAuth 在您的组件中,您可以使用useSession钩子来获取当前会话状态:\n1 2 3 4 5 6 7 8 9 10 11 import { useSession } from \u0026#34;next-auth/react\u0026#34;; export default function Component() { const { data: session } = useSession(); if (session) { return \u0026lt;p\u0026gt;Welcome, {session.user.name}!\u0026lt;/p\u0026gt;; } return \u0026lt;p\u0026gt;Please sign in\u0026lt;/p\u0026gt;; } 步骤9: 保护路由 您可以使用getServerSession函数来保护服务器端路由:\n1 2 3 4 5 6 7 8 9 10 11 12 import { getServerSession } from \u0026#34;next-auth/next\u0026#34;; import { authOptions } from \u0026#34;@/lib/auth\u0026#34;; export default async function ProtectedPage() { const session = await getServerSession(authOptions); if (!session) { return \u0026lt;p\u0026gt;Access Denied\u0026lt;/p\u0026gt;; } return \u0026lt;p\u0026gt;Welcome to the protected page, {session.user.name}!\u0026lt;/p\u0026gt;; } 这就是在Next.js项目中设置和配置NextAuth的基本步骤。根据您的具体需求,您可能需要进行更多自定义配置。\n","date":"2024-10-20T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/nextauth%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/cover_hu_776dd98055d6b43c.png","permalink":"https://woniu9524.github.io/woniublog/p/nextauth%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/","title":"NextAuth配置教程"},{"content":"Git Revert 使用指南 使用revert撤销一个提交\n什么是 Git Revert？ Git revert 是一个用于撤销之前的某个提交的命令。与 git reset 不同，git revert 不会删除任何历史记录，而是通过创建一个新的提交来撤销指定提交的更改。\n为什么使用 Revert？ 安全：不改变提交历史，适合在共享分支上使用。 可追踪：撤销操作本身被记录为一个新的提交。 灵活：可以撤销任何历史提交，而不仅仅是最近的几个。 如何使用 Revert 基本用法 找到要撤销的提交的 hash：\n1 git log --oneline 执行 revert 命令：\n1 git revert \u0026lt;commit-hash\u0026gt; 解决可能出现的冲突（如果有的话）。\n提交 revert 更改：\n1 git commit -m \u0026#34;Revert \u0026#39;xxx\u0026#39;\u0026#34; 示例 假设我们有以下提交历史：\n1 2 3 abc1234 (HEAD -\u0026gt; main) Add feature C def5678 Add feature B (要撤销的提交) ghi9101 Add feature A 要撤销 \u0026ldquo;Add feature B\u0026rdquo; 的提交，我们可以：\n1 git revert def5678 这将创建一个新的提交，撤销 \u0026ldquo;Add feature B\u0026rdquo; 的更改。\n撤销多个提交 要撤销多个连续的提交，可以使用：\n1 git revert HEAD~3..HEAD 这将撤销最近的三个提交。\n只创建 Revert 提交而不自动提交 如果你想在提交前检查 revert 的结果：\n1 git revert -n \u0026lt;commit-hash\u0026gt; 这会将更改添加到工作目录和暂存区，但不会自动创建新的提交。\n注意事项 确保在执行 revert 前，你的工作目录是干净的。 如果 revert 的提交与之后的提交有冲突，你可能需要手动解决这些冲突。 对于已经推送到远程仓库的提交，使用 revert 比使用 reset 更安全。 git revert 操作时，Git 不允许在有未提交更改的情况下执行 revert，以防止你的本地修改被意外覆盖。 ","date":"2024-10-15T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/git-revert-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/cover_hu_fbbb3744a166f4dd.jpg","permalink":"https://woniu9524.github.io/woniublog/p/git-revert-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","title":"Git Revert 使用指南"},{"content":" fabric绘图，分区，区域填充，图形属性获取\n需求背景 需要对老鼠行为进行分析，例如老鼠在视频中的哪些区域范围活动。所以需要用fabric在视频上增加一个画布，然后在画布上进行分区。根据老鼠坐标变化来确认当前老鼠在画布中的位置。\n遇到的一些问题 fabric应该是不提供填充功能的，我记得之前看issue有人提过，但是作者说不提供这个功能，所以要自己实现。我是先通过洪水填充的办法在画布上用canvas绘制填充区域，然后把它作为图片添加到fabric画布上。 解析fabric的时候自定义属性不能被序列化。因为要给填充区域设置标题，还有圆形矩形比例尺啊这些设置名称，我需自定义一些名称。issue里面有解决方案 关键的一些代码 自定义属性\n1 2 3 4 5 fabric.Object.prototype.toObject = (function (toObject) { return function (propertiesToInclude) { return toObject.call(this, [\u0026#39;name\u0026#39;, \u0026#39;title\u0026#39;].concat(propertiesToInclude)); }; })(fabric.Object.prototype.toObject); 洪水填充\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 import { fabric } from \u0026#39;fabric\u0026#39;; class FloodFillTool { constructor(fcanvas, fillColor = \u0026#39;#f00\u0026#39;, name = \u0026#39;未定义\u0026#39;, fillTolerance = 2) { this.fcanvas = fcanvas; this.fillColor = fillColor; this.fillTolerance = fillTolerance; this.name = name; } static hexToRgb(hex, opacity) { opacity = Math.round(opacity * 255) || 255; hex = hex.replace(\u0026#39;#\u0026#39;, \u0026#39;\u0026#39;); const rgb = []; const re = new RegExp(\u0026#39;(.{\u0026#39; + hex.length / 3 + \u0026#39;})\u0026#39;, \u0026#39;g\u0026#39;); hex.match(re).map(function (l) { rgb.push(parseInt(hex.length % 2 ? l + l : l, 16)); }); return rgb.concat(opacity); } static withinTolerance(array1, offset, array2, tolerance) { let length = array2.length; let start = offset + length; tolerance = tolerance || 0; while (start-- \u0026amp;\u0026amp; length--) { if (Math.abs(array1[start] - array2[length]) \u0026gt; tolerance) { return false; } } return true; } static fill(imageData, getPointOffsetFn, point, color, target, tolerance, width, height) { const directions = [ [1, 0], [0, 1], [0, -1], [-1, 0] ]; let coords = []; let points = [point]; let seen = {}; let key, x, y, offset, i, x2, y2; let minX = -1, maxX = -1, minY = -1, maxY = -1; while (!!(point = points.pop())) { x = point.x; y = point.y; offset = getPointOffsetFn(x, y); if (!FloodFillTool.withinTolerance(imageData, offset, target, tolerance)) { continue; } if (x \u0026gt; maxX) { maxX = x; } if (y \u0026gt; maxY) { maxY = y; } if (x \u0026lt; minX || minX == -1) { minX = x; } if (y \u0026lt; minY || minY == -1) { minY = y; } i = directions.length; while (i--) { if (i \u0026lt; 4) { imageData[offset + i] = color[i]; coords[offset + i] = color[i]; } x2 = x + directions[i][0]; y2 = y + directions[i][1]; key = x2 + \u0026#39;,\u0026#39; + y2; if (x2 \u0026lt; 0 || y2 \u0026lt; 0 || x2 \u0026gt;= width || y2 \u0026gt;= height || seen[key]) { continue; } points.push({ x: x2, y: y2 }); seen[key] = true; } } return { x: minX, y: minY, width: maxX - minX, height: maxY - minY, coords: coords }; } enableFloodFill(enable) { if (!enable) { this.fcanvas.off(\u0026#39;mouse:down\u0026#39;); this.fcanvas.selection = true; this.fcanvas.forEachObject((object) =\u0026gt; { object.selectable = true; }); return; } this.fcanvas.discardActiveObject().renderAll(); this.fcanvas.selection = false; this.fcanvas.forEachObject((object) =\u0026gt; { object.selectable = false; }); this.fcanvas.on({ \u0026#39;mouse:down\u0026#39;: (e) =\u0026gt; { const mouse = this.fcanvas.getPointer(e.e); const mouseX = Math.round(mouse.x), mouseY = Math.round(mouse.y); const canvas = this.fcanvas.lowerCanvasEl; const context = canvas.getContext(\u0026#39;2d\u0026#39;); const parsedColor = FloodFillTool.hexToRgb(this.fillColor, 0.6); const imageData = context.getImageData(0, 0, canvas.width, canvas.height); const getPointOffset = (x, y) =\u0026gt; 4 * (y * imageData.width + x); const targetOffset = getPointOffset(mouseX, mouseY); const target = imageData.data.slice(targetOffset, targetOffset + 4); if (FloodFillTool.withinTolerance(target, 0, parsedColor, this.fillTolerance)) { console.log(\u0026#39;Ignore... same color\u0026#39;); return; } const data = FloodFillTool.fill( imageData.data, getPointOffset, { x: mouseX, y: mouseY }, parsedColor, target, this.fillTolerance, imageData.width, imageData.height ); if (0 === data.width || 0 === data.height) { return; } const tmpCanvas = document.createElement(\u0026#39;canvas\u0026#39;); const tmpCtx = tmpCanvas.getContext(\u0026#39;2d\u0026#39;); tmpCanvas.width = canvas.width; tmpCanvas.height = canvas.height; const palette = tmpCtx.getImageData(0, 0, tmpCanvas.width, tmpCanvas.height); palette.data.set(new Uint8ClampedArray(data.coords)); tmpCtx.putImageData(palette, 0, 0); const imgData = tmpCtx.getImageData(data.x, data.y, data.width, data.height); tmpCanvas.width = data.width; tmpCanvas.height = data.height; tmpCtx.putImageData(imgData, 0, 0); const img = new Image(); img.onload = () =\u0026gt; { this.fcanvas.add( new fabric.Image(img, { left: data.x, top: data.y, selectable: false, name: this.name, title: \u0026#39;FloodFill\u0026#39; }) ); }; img.src = tmpCanvas.toDataURL(\u0026#39;image/png\u0026#39;); } }); } } export default FloodFillTool; ","date":"2024-08-16T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/fabric%E7%BB%98%E5%88%B6%E5%9B%BE%E5%BD%A2%E5%A1%AB%E5%85%85%E5%8C%BA%E5%9F%9F%E5%92%8C%E6%A3%80%E6%B5%8B/cover_hu_e20b8c0e5a436b9c.png","permalink":"https://woniu9524.github.io/woniublog/p/fabric%E7%BB%98%E5%88%B6%E5%9B%BE%E5%BD%A2%E5%A1%AB%E5%85%85%E5%8C%BA%E5%9F%9F%E5%92%8C%E6%A3%80%E6%B5%8B/","title":"Fabric绘制图形，填充区域和检测"},{"content":" 切片上传+断点续传+并发上传+ffmpeg后端压缩\n需求背景 我需要把实验视频上传到服务器上然后进行分析。分析视频不需要高清的视频，所以需要对视频进行压缩。开始尝试在前端进行压缩视频，测试发现不是很靠谱，遂放弃，转用前端上传，到后端压缩。\n因为网站使用cloudflare进行反向代理，可能cf在国内网络比较差的原因，导致网络十分不稳定，大文件没有办法上传。\n处理流程 前端先计算视频的md5值；\n接着带着md5请求后端，是否有视频碎片(为了能断点续传)；\n接着对视频切片，跳过已经上传了的碎片，此处因为网络原因，我做了网络错误重试；\n我设置三个一组，并发上传，使用Promise.all来控制；\n在切片都完成上传后，发送合并请求；\n请求后因为后端要合并和压缩视频，时间比较长，很容易造成网络中断，所以轮询请求获取当前上传状态。\n视频合并和压缩，完成后，删除本地文件，把状态写入redis等待轮询；\n过程中遇到的一些问题 网络问题：视频上传的过程中会因为网络问题照成中断，我对上传进行了错误重传。压缩过程比较长也有同样的问题，我使用轮询的方式获取状态。\n视频合并OOM：起初把视频读取到内存中进行合并，这样非常危险，很容易照成OOM。后面改成流式处理，设置固定大小的缓冲区。\n代码 前端\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 \u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div v-if=\u0026#34;videoUrl\u0026#34; class=\u0026#34;video-container\u0026#34;\u0026gt; \u0026lt;video :src=\u0026#34;videoUrl\u0026#34; class=\u0026#34;video-player\u0026#34; controls\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;upload-container\u0026#34;\u0026gt; \u0026lt;el-upload :auto-upload=\u0026#34;false\u0026#34; :before-upload=\u0026#34;beforeUpload\u0026#34; :file-list=\u0026#34;fileList\u0026#34; :limit=\u0026#34;1\u0026#34; :on-change=\u0026#34;handleChange\u0026#34; :on-exceed=\u0026#34;handleExceed\u0026#34; :on-preview=\u0026#34;handlePreview\u0026#34; :on-remove=\u0026#34;handleRemove\u0026#34; accept=\u0026#34;video/*\u0026#34; class=\u0026#34;upload-demo\u0026#34; drag \u0026gt; \u0026lt;el-icon class=\u0026#34;el-icon--upload\u0026#34;\u0026gt; \u0026lt;UploadFilled /\u0026gt; \u0026lt;/el-icon\u0026gt; \u0026lt;div class=\u0026#34;el-upload__text\u0026#34;\u0026gt;拖拽视频到此处，或\u0026lt;em\u0026gt;点击上传\u0026lt;/em\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/el-upload\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;el-dialog v-model=\u0026#34;dialogVisible\u0026#34; :close-on-click-modal=\u0026#34;false\u0026#34; title=\u0026#34;处理进度\u0026#34; width=\u0026#34;30%\u0026#34;\u0026gt; \u0026lt;el-progress :percentage=\u0026#34;progressPercentage\u0026#34; :status=\u0026#34;progressStatus\u0026#34;\u0026gt;\u0026lt;/el-progress\u0026gt; \u0026lt;div\u0026gt;{{ progressText }}\u0026lt;/div\u0026gt; \u0026lt;/el-dialog\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script lang=\u0026#34;ts\u0026#34; setup\u0026gt; import { ref, onMounted } from \u0026#39;vue\u0026#39;; import { ElMessage } from \u0026#39;element-plus\u0026#39;; import { UploadFilled } from \u0026#39;@element-plus/icons-vue\u0026#39;; import { getMergeChunksStatus, getUploadedChunks, getVideoUrl, mergeChunksForExperimentVideo, uploadChunk } from \u0026#39;@/api/lab/content/labContent\u0026#39;; import { useDrawingStore } from \u0026#39;@/store/modules/drawing\u0026#39;; import SparkMD5 from \u0026#39;spark-md5\u0026#39;; import axios from \u0026#39;axios\u0026#39;; // 常量定义 const CHUNK_SIZE = 5 * 1024 * 1024; // 每个分片5MB const MAX_RETRIES = 3; // 最大重试次数 const RETRY_DELAY = 2000; // 重试延迟时间（毫秒） const MAX_FILE_SIZE = 2 * 1024 * 1024 * 1024; // 最大文件大小（2GB） const CONCURRENT_UPLOADS = 3; // 并发上传数量 // 状态管理 const drawingStore = useDrawingStore(); // 响应式变量 const fileList = ref([]); const videoUrl = ref(\u0026#39;\u0026#39;); const dialogVisible = ref(false); const progressPercentage = ref(0); const progressStatus = ref(\u0026#39;\u0026#39;); const progressText = ref(\u0026#39;\u0026#39;); // 更新进度信息 const updateProgress = (percentage: number, text: string, status: string = \u0026#39;\u0026#39;) =\u0026gt; { progressPercentage.value = Math.min(100, Math.max(0, Number(percentage.toFixed(2)))); progressText.value = text; progressStatus.value = status; }; // 获取文件分片 const getFileChunks = (fileSize: number) =\u0026gt; { const chunks = []; let start = 0; while (start \u0026lt; fileSize) { const end = Math.min(start + CHUNK_SIZE, fileSize); chunks.push({ start, end }); start = end; } return chunks; }; // 计算文件MD5 const computeMD5 = (file: File): Promise\u0026lt;string\u0026gt; =\u0026gt; { return new Promise((resolve, reject) =\u0026gt; { const spark = new SparkMD5.ArrayBuffer(); const reader = new FileReader(); const chunks = getFileChunks(file.size); let currentChunk = 0; reader.onload = (e: any) =\u0026gt; { spark.append(e.target.result); currentChunk++; if (currentChunk \u0026lt; chunks.length) { loadNext(); } else { resolve(spark.end()); } }; reader.onerror = (error) =\u0026gt; { console.error(error); reject(\u0026#39;MD5计算失败\u0026#39;); }; const loadNext = () =\u0026gt; { const { start, end } = chunks[currentChunk]; reader.readAsArrayBuffer(file.slice(start, end)); }; loadNext(); }); }; // 清理上传状态 const clearUpload = () =\u0026gt; { fileList.value = []; videoUrl.value = \u0026#39;\u0026#39;; }; // 延迟执行 const sleep = (ms: number) =\u0026gt; new Promise((resolve) =\u0026gt; setTimeout(resolve, ms)); // 带重试的操作执行 const retryOperation = async (operation: () =\u0026gt; Promise\u0026lt;any\u0026gt;, retries = MAX_RETRIES) =\u0026gt; { try { return await operation(); } catch (error) { if (retries \u0026gt; 0 \u0026amp;\u0026amp; axios.isAxiosError(error)) { console.log(`操作重试中，剩余尝试次数：${retries}`); await sleep(RETRY_DELAY); return retryOperation(operation, retries - 1); } throw error; } }; // 轮询合并状态 const pollMergeStatus = async (md5: string) =\u0026gt; { while (true) { try { const response = await getMergeChunksStatus(md5); const status = response.msg; if (status.includes(\u0026#39;error\u0026#39;)) { throw new Error(\u0026#39;合并失败: \u0026#39; + status); } else if (status.includes(\u0026#39;finish\u0026#39;)) { return response.msg; } // 等待一段时间后再次轮询 await sleep(3000); } catch (error) { console.error(\u0026#39;轮询合并状态时出错:\u0026#39;, error); throw error; } } }; // 文件上传超出限制处理 const handleExceed = (files: File[], fileList: File[]) =\u0026gt; { ElMessage.warning(\u0026#39;只能上传一个视频文件。\u0026#39;); }; // 文件预览处理 const handlePreview = (file: File) =\u0026gt; { console.log(\u0026#39;预览文件\u0026#39;, file); }; // 文件移除处理 const handleRemove = (file: File, fileList: File[]) =\u0026gt; { console.log(\u0026#39;移除文件\u0026#39;, file, fileList); }; // 上传前的文件检查 const beforeUpload = (file: File) =\u0026gt; { const isVideo = file.type.startsWith(\u0026#39;video/\u0026#39;); const isLt2G = file.size \u0026lt;= MAX_FILE_SIZE; if (!isVideo) { ElMessage.error(\u0026#39;只能上传视频文件！\u0026#39;); return false; } if (!isLt2G) { ElMessage.error(\u0026#39;视频大小不能超过 2GB!\u0026#39;); return false; } return true; }; // 文件状态改变处理 const handleChange = async (file: any, fileList: any) =\u0026gt; { if (file.status === \u0026#39;ready\u0026#39;) { dialogVisible.value = true; updateProgress(0, \u0026#39;准备上传视频...\u0026#39;); try { const md5 = await computeMD5(file.raw); const chunks = getFileChunks(file.raw.size); const uploadedChunksResponse = await retryOperation(() =\u0026gt; getUploadedChunks(md5)); if (!uploadedChunksResponse.data) { throw new Error(\u0026#39;服务器响应无效\u0026#39;); } const uploadedChunks = uploadedChunksResponse.data; if (typeof uploadedChunks === \u0026#39;boolean\u0026#39; \u0026amp;\u0026amp; uploadedChunks) { updateProgress(100, \u0026#39;文件已存在，跳过上传\u0026#39;, \u0026#39;success\u0026#39;); ElMessage.success(\u0026#39;文件已存在，上传成功\u0026#39;); setTimeout(() =\u0026gt; { dialogVisible.value = false; window.location.reload(); }, 1500); return; } if (!Array.isArray(uploadedChunks)) { throw new Error(\u0026#39;服务器返回的数据格式不正确\u0026#39;); } const chunksToUpload = chunks.filter((chunk, index) =\u0026gt; !uploadedChunks.includes(index)); const totalChunks = chunksToUpload.length; let uploadedCount = 0; // 并发上传函数 const uploadChunkConcurrently = async (chunk: { start: number; end: number }, index: number) =\u0026gt; { const { start, end } = chunk; const formData = new FormData(); const blob = file.raw.slice(start, end); if (blob.size === 0) { console.warn(\u0026#39;遇到空分片，跳过...\u0026#39;); return; } formData.append(\u0026#39;file\u0026#39;, blob, `${file.raw.name}.part${index}`); formData.append(\u0026#39;md5\u0026#39;, md5); formData.append(\u0026#39;chunkIndex\u0026#39;, index.toString()); await retryOperation(() =\u0026gt; uploadChunk(formData)); uploadedCount++; updateProgress((uploadedCount / totalChunks) * 90, `正在上传 ${uploadedCount}/${totalChunks}`); }; // 使用 Promise.all 和 Array.slice 来控制并发量 for (let i = 0; i \u0026lt; chunksToUpload.length; i += CONCURRENT_UPLOADS) { const uploadPromises = chunksToUpload.slice(i, i + CONCURRENT_UPLOADS).map((chunk, index) =\u0026gt; uploadChunkConcurrently(chunk, i + index) ); await Promise.all(uploadPromises); } updateProgress(95, \u0026#39;正在合并压缩视频...\u0026#39;); try { await mergeChunksForExperimentVideo(md5, file.raw.name, drawingStore.currentExperimentId); } catch (e) { console.error(e); } const result = await pollMergeStatus(md5); if (result) { updateProgress(100, \u0026#39;上传成功\u0026#39;, \u0026#39;success\u0026#39;); ElMessage.success(\u0026#39;上传成功\u0026#39;); drawingStore.getStepStatus(drawingStore.currentExperimentId); setTimeout(() =\u0026gt; { dialogVisible.value = false; window.location.reload(); }, 1500); } else { throw new Error(\u0026#39;上传失败\u0026#39;); } } catch (error) { console.error(\u0026#39;上传错误\u0026#39;, error); updateProgress(100, \u0026#39;上传失败\u0026#39;, \u0026#39;exception\u0026#39;); if (axios.isAxiosError(error) \u0026amp;\u0026amp; error.response?.status === 502) { ElMessage.error(`上传失败: 服务器暂时无法响应，请稍后重试`); } else { ElMessage.error(`上传失败: ${error.msg || \u0026#39;未知错误\u0026#39;}`); } clearUpload(); } } }; // 生命周期钩子 onMounted(() =\u0026gt; { getVideoUrl(drawingStore.currentExperimentId).then((response) =\u0026gt; { videoUrl.value = response.data?.url; }); }); \u0026lt;/script\u0026gt; \u0026lt;style scoped\u0026gt; .container { display: flex; justify-content: space-between; align-items: flex-start; gap: 20px; padding: 20px; background-color: #f0f2f5; border-radius: 8px; box-shadow: 0 2px 12px rgba(0, 0, 0, 0.1); } .video-container { flex: 1; max-width: 60%; } .video-player { width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2); } .upload-container { flex: 1; max-width: 35%; } .upload-demo { border: 1px dashed #d9d9d9; border-radius: 6px; background-color: #ffffff; text-align: center; cursor: pointer; overflow: hidden; position: relative; padding: 20px; transition: border-color 0.3s; } .upload-demo:hover { border-color: #409eff; } .el-upload__text { color: #606266; font-size: 14px; margin-top: 16px; } .el-upload__tip { color: #909399; font-size: 12px; margin-top: 6px; } \u0026lt;/style\u0026gt; 后端\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 public class UploadController { @Autowired private ISysOssService ossService; @Autowired private SysOssMapper ossMapper; // 使用系统临时目录作为基础路径 private static final String BASE_DIR = System.getProperty(\u0026#34;java.io.tmpdir\u0026#34;); // 临时存储上传分片的目录 private static final String CHUNK_FOLDER = \u0026#34;uploads\u0026#34; + File.separator + \u0026#34;chunks\u0026#34; + File.separator; // 合并后的视频临时存储目录 private static final String MERGED_FOLDER = \u0026#34;uploads\u0026#34; + File.separator + \u0026#34;merged\u0026#34; + File.separator; /** * 获取已上传的分片信息 */ @GetMapping(\u0026#34;/getUploadedChunks\u0026#34;) public R\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; getUploadedChunks(@RequestParam String md5) { Path chunkDir = Paths.get(BASE_DIR, CHUNK_FOLDER, md5); if (!Files.exists(chunkDir)) { return R.ok(new ArrayList\u0026lt;\u0026gt;()); } List\u0026lt;Integer\u0026gt; uploadedChunks = new ArrayList\u0026lt;\u0026gt;(); try (DirectoryStream\u0026lt;Path\u0026gt; stream = Files.newDirectoryStream(chunkDir)) { for (Path path : stream) { uploadedChunks.add(Integer.parseInt(path.getFileName().toString())); } } catch (IOException e) { return R.fail(\u0026#34;获取已上传分片失败: \u0026#34; + e.getMessage()); } return R.ok(uploadedChunks); } /** * 上传单个分片 */ @Log(title = \u0026#34;上传视频分片\u0026#34;, businessType = BusinessType.INSERT) @PostMapping(\u0026#34;/uploadChunk\u0026#34;) public R\u0026lt;Void\u0026gt; uploadChunk(@RequestParam(\u0026#34;file\u0026#34;) MultipartFile file, @RequestParam(\u0026#34;md5\u0026#34;) String md5, @RequestParam(\u0026#34;chunkIndex\u0026#34;) int chunkIndex) { if (file.isEmpty()) { return R.fail(\u0026#34;上传文件不能为空\u0026#34;); } Path chunkDir = Paths.get(BASE_DIR, CHUNK_FOLDER, md5); try { Files.createDirectories(chunkDir); Path chunkPath = chunkDir.resolve(String.valueOf(chunkIndex)); Files.copy(file.getInputStream(), chunkPath, StandardCopyOption.REPLACE_EXISTING); return R.ok(); } catch (IOException e) { return R.fail(\u0026#34;分片上传失败: \u0026#34; + e.getMessage()); } } /** * 合并所有分片 */ /** * 合并所有分片 */ @Log(title = \u0026#34;合并视频分片\u0026#34;, businessType = BusinessType.INSERT) @PostMapping(\u0026#34;/mergeChunks/experimentVideo\u0026#34;) public R\u0026lt;SysOssUploadVo\u0026gt; mergeChunksExperimentVideo(@RequestParam(\u0026#34;md5\u0026#34;) String md5, @RequestParam(\u0026#34;filename\u0026#34;) String filename, @RequestParam(\u0026#34;experimentId\u0026#34;) Long experimentId) { // 获取分片目录路径 Path chunkDir = Paths.get(BASE_DIR, CHUNK_FOLDER, md5); if (!Files.exists(chunkDir)) { return R.fail(\u0026#34;没有找到上传的分片\u0026#34;); } Path mergedFile = Paths.get(BASE_DIR, MERGED_FOLDER, md5, filename); Path compressedFile = Paths.get(BASE_DIR, MERGED_FOLDER, md5, \u0026#34;compressed_\u0026#34; + filename); try { Files.createDirectories(mergedFile.getParent()); RedisUtils.setCacheObject(CacheConstants.VIDEO_COMPRESS_STATUS_KEY + md5, \u0026#34;start compress\u0026#34;, Duration.ofMinutes(60)); Integer testDuration = ossMapper.queryTestDuration(experimentId); FFmpegUtils.mergeAndCompressVideo(chunkDir, mergedFile, compressedFile, testDuration); // 创建 MultipartFile 对象 MultipartFile multipartFile; try (InputStream inputStream = Files.newInputStream(compressedFile)) { multipartFile = new MockMultipartFile(filename, filename, Files.probeContentType(compressedFile), IOUtils.toByteArray(inputStream)); } // 上传到 OSS SysOssVo oss = ossService.upload(multipartFile, experimentId); // 创建上传结果对象 SysOssUploadVo uploadVo = new SysOssUploadVo(); uploadVo.setUrl(oss.getUrl()); uploadVo.setFileName(oss.getOriginalName()); uploadVo.setOssId(oss.getOssId().toString()); // 保存 ossId 到实验 ossService.saveOssId(experimentId, oss.getOssId()); // 清理临时文件 deleteDirectory(chunkDir); deleteDirectory(mergedFile.getParent()); // redis 设置状态: 完成上傳 RedisUtils.setCacheObject(CacheConstants.VIDEO_COMPRESS_STATUS_KEY+md5,\u0026#34;finish upload\u0026#34;, Duration.ofMinutes(60)); return R.ok(uploadVo); } catch (IOException e) { // redis 设置状态: 壓縮失敗 RedisUtils.setCacheObject(CacheConstants.VIDEO_COMPRESS_STATUS_KEY+md5,\u0026#34;compress video error\u0026#34;, Duration.ofMinutes(60)); // 合并失败时，清理临时文件和目录 try { deleteDirectory(mergedFile.getParent()); deleteDirectory(chunkDir); } catch (IOException cleanupException) { log.error(\u0026#34;清理临时文件失败\u0026#34;, cleanupException); } return R.fail(\u0026#34;合并分片失败: \u0026#34; + e.getMessage()); } catch (InterruptedException e) { throw new RuntimeException(e); } } /* * 獲取當前視頻上傳狀態 * */ @GetMapping(\u0026#34;/getMergeChunksStatus/{md5}\u0026#34;) public R\u0026lt;String\u0026gt; getMergeChunksStatus(@PathVariable(\u0026#34;md5\u0026#34;) String md5) { String status = RedisUtils.getCacheObject(CacheConstants.VIDEO_COMPRESS_STATUS_KEY+md5); if (status == null) { return R.ok(\u0026#34;no status\u0026#34;); } else { return R.ok(status); } } /** * 递归删除目录 */ private void deleteDirectory(Path directory) throws IOException { Files.walkFileTree(directory, new SimpleFileVisitor\u0026lt;Path\u0026gt;() { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { Files.delete(file); return FileVisitResult.CONTINUE; } @Override public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException { Files.delete(dir); return FileVisitResult.CONTINUE; } }); } } ffmpeg\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 package org.dromara.common.core.utils.file; import lombok.extern.slf4j.Slf4j; import org.bytedeco.ffmpeg.global.avutil; import org.springframework.stereotype.Component; import org.bytedeco.javacv.*; import org.bytedeco.ffmpeg.global.avcodec; import java.io.*; import java.nio.file.Files; import java.nio.file.Path; import java.util.List; @Slf4j @Component public class FFmpegUtils { public static void mergeAndCompressVideo(Path chunkDir, Path mergedFile, Path compressedFile, Integer testDuration) throws IOException, FrameGrabber.Exception, FrameRecorder.Exception { log.info(\u0026#34;开始合并和压缩视频: 输入目录 = {}, 合并文件 = {}, 压缩文件 = {}, 测试时长 = {} 秒\u0026#34;, chunkDir, mergedFile, compressedFile, testDuration); // 合并分片 mergeChunks(chunkDir, mergedFile); log.info(\u0026#34;视频合并完成\u0026#34;); // 压缩视频 compressVideo(mergedFile.toString(), compressedFile.toString(), testDuration); log.info(\u0026#34;视频合并和压缩成功完成\u0026#34;); } private static void mergeChunks(Path chunkDir, Path mergedFile) throws IOException { List\u0026lt;Path\u0026gt; chunks; try (var pathStream = Files.list(chunkDir)) { chunks = pathStream .sorted((p1, p2) -\u0026gt; { int i1 = Integer.parseInt(p1.getFileName().toString()); int i2 = Integer.parseInt(p2.getFileName().toString()); return Integer.compare(i1, i2); }) .toList(); } int bufferSize = 16 * 1024 * 1024; // 128MB 缓冲区 try (OutputStream out = new BufferedOutputStream(Files.newOutputStream(mergedFile))) { byte[] buffer = new byte[bufferSize]; int bytesRead; for (Path chunk : chunks) { try (InputStream in = new BufferedInputStream(Files.newInputStream(chunk))) { while ((bytesRead = in.read(buffer)) != -1) { out.write(buffer, 0, bytesRead); } } } } } static { // 设置 FFmpeg 日志回调 avutil.av_log_set_level(avutil.AV_LOG_ERROR); FFmpegLogCallback.set(); } public static void compressVideo(String inputFilePath, String outputFilePath, Integer testDuration) throws IOException { log.info(\u0026#34;开始压缩视频: 输入文件 = {}, 输出文件 = {}, 测试时长 = {} 秒\u0026#34;, inputFilePath, outputFilePath, testDuration); try (FFmpegFrameGrabber grabber = new FFmpegFrameGrabber(inputFilePath)) { grabber.start(); log.info(\u0026#34;输入视频信息: 宽度 = {}, 高度 = {}, 帧率 = {}, 像素格式 = {}\u0026#34;, grabber.getImageWidth(), grabber.getImageHeight(), grabber.getFrameRate(), grabber.getPixelFormat()); try (FFmpegFrameRecorder recorder = new FFmpegFrameRecorder(outputFilePath, grabber.getImageWidth(), grabber.getImageHeight())) { recorder.setVideoCodec(avcodec.AV_CODEC_ID_H264); recorder.setFormat(\u0026#34;mp4\u0026#34;); recorder.setFrameRate(30); recorder.setFrameRate(grabber.getFrameRate()); recorder.setVideoBitrate(300000); recorder.setVideoOption(\u0026#34;preset\u0026#34;, \u0026#34;medium\u0026#34;); // 将预设改为 \u0026#34;medium\u0026#34; 以平衡压缩速度和质量 recorder.setVideoOption(\u0026#34;crf\u0026#34;, \u0026#34;23\u0026#34;); recorder.setVideoOption(\u0026#34;threads\u0026#34;, \u0026#34;auto\u0026#34;); recorder.start(); Frame frame; long startTime = System.currentTimeMillis(); long duration = (testDuration != null \u0026amp;\u0026amp; testDuration \u0026gt; 0) ? testDuration * 1000L : Long.MAX_VALUE; while ((frame = grabber.grab()) != null) { if (System.currentTimeMillis() - startTime \u0026gt; duration) { break; } recorder.record(frame); } } } catch (Exception e) { log.error(\u0026#34;视频压缩过程中发生错误\u0026#34;, e); throw e; } log.info(\u0026#34;视频压缩完成: 输出文件 = {}\u0026#34;, outputFilePath); } } ","date":"2024-08-15T00:00:00Z","image":"https://woniu9524.github.io/woniublog/p/%E5%A4%A7%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E8%A7%86%E9%A2%91%E5%8E%8B%E7%BC%A9%E4%B8%8A%E4%BC%A0/cover_hu_d766461f6097e82b.png","permalink":"https://woniu9524.github.io/woniublog/p/%E5%A4%A7%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E8%A7%86%E9%A2%91%E5%8E%8B%E7%BC%A9%E4%B8%8A%E4%BC%A0/","title":"大文件上传(视频压缩上传)"}]